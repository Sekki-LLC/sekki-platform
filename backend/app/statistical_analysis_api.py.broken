# statistical_analysis_api.py

import pandas as pd
import numpy as np
import json
import io
import base64
from flask import Blueprint, request, jsonify, current_app
from werkzeug.utils import secure_filename
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import shapiro, normaltest, kstest, anderson
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from openai import OpenAI
import os

# Create blueprint
statistical_bp = Blueprint('statistical_analysis', __name__, url_prefix='/api/statistical-analysis')

# Configure OpenAI

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in {'csv', 'xlsx', 'xls'}

def detect_column_types(df):
    """Detect and classify column types"""
    column_info = []
    for col in df.columns:
        if df[col].dtype in ['int64', 'float64']:
            col_type = 'numeric'
        elif df[col].dtype == 'object':
            # Check if it's categorical or text
            unique_ratio = df[col].nunique() / len(df)
            if unique_ratio < 0.1:  # Less than 10% unique values
                col_type = 'categorical'
            else:
                col_type = 'text'
        else:
            col_type = 'other'
        
        column_info.append({
            'name': col,
            'type': col_type,
            'unique_count': df[col].nunique(),
            'null_count': df[col].isnull().sum()
        })
    
    return column_info

def calculate_descriptive_stats(df, numeric_columns):
    """Calculate comprehensive descriptive statistics"""
    stats_dict = {}
    
    for col in numeric_columns:
        data = df[col].dropna()
        
        stats_dict[col] = {
            'count': len(data),
            'mean': float(data.mean()),
            'median': float(data.median()),
            'std': float(data.std()),
            'var': float(data.var()),
            'min': float(data.min()),
            'max': float(data.max()),
            'q1': float(data.quantile(0.25)),
            'q3': float(data.quantile(0.75)),
            'iqr': float(data.quantile(0.75) - data.quantile(0.25)),
            'skewness': float(stats.skew(data)),
            'kurtosis': float(stats.kurtosis(data)),
            'range': float(data.max() - data.min())
        }
    
    return stats_dict

def perform_normality_tests(df, numeric_columns):
    """Perform various normality tests"""
    normality_results = {}
    
    for col in numeric_columns:
        data = df[col].dropna()
        
        if len(data) < 3:
            continue
            
        results = {}
        
        # Shapiro-Wilk test
        if len(data) <= 5000:  # Shapiro-Wilk has sample size limitations
            shapiro_stat, shapiro_p = shapiro(data)
            results['shapiro_wilk'] = {
                'statistic': float(shapiro_stat),
                'p_value': float(shapiro_p),
                'is_normal': shapiro_p > 0.05
            }
        
        # D'Agostino's normality test
        if len(data) >= 8:
            dagostino_stat, dagostino_p = normaltest(data)
            results['dagostino'] = {
                'statistic': float(dagostino_stat),
                'p_value': float(dagostino_p),
                'is_normal': dagostino_p > 0.05
            }
        
        # Anderson-Darling test
        anderson_result = anderson(data, dist='norm')
        results['anderson_darling'] = {
            'statistic': float(anderson_result.statistic),
            'critical_values': anderson_result.critical_values.tolist(),
            'significance_levels': anderson_result.significance_level.tolist()
        }
        
        normality_results[col] = results
    
    return normality_results

def perform_correlation_analysis(df, numeric_columns):
    """Perform correlation analysis"""
    if len(numeric_columns) < 2:
        return {}
    
    numeric_df = df[numeric_columns]
    
    # Pearson correlation
    pearson_corr = numeric_df.corr(method='pearson')
    
    # Spearman correlation
    spearman_corr = numeric_df.corr(method='spearman')
    
    return {
        'pearson': pearson_corr.to_dict(),
        'spearman': spearman_corr.to_dict()
    }

def generate_visualizations(df, numeric_columns, categorical_columns):
    """Generate statistical visualizations"""
    charts = []
    
    # Set style
    plt.style.use('default')
    sns.set_palette("husl")
    
    for col in numeric_columns[:3]:  # Limit to first 3 numeric columns
        # Histogram
        fig, ax = plt.subplots(figsize=(8, 6))
        ax.hist(df[col].dropna(), bins=30, alpha=0.7, edgecolor='black')
        ax.set_title(f'Distribution of {col}')
        ax.set_xlabel(col)
        ax.set_ylabel('Frequency')
        
        # Save to base64
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()
        plt.close()
        
        charts.append({
            'title': f'Histogram: {col}',
            'type': 'histogram',
            'url': f'data:image/png;base64,{image_base64}'
        })
        
        # Box plot
        fig, ax = plt.subplots(figsize=(8, 6))
        ax.boxplot(df[col].dropna())
        ax.set_title(f'Box Plot of {col}')
        ax.set_ylabel(col)
        
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()
        plt.close()
        
        charts.append({
            'title': f'Box Plot: {col}',
            'type': 'boxplot',
            'url': f'data:image/png;base64,{image_base64}'
        })
    
    # Correlation heatmap if multiple numeric columns
    if len(numeric_columns) > 1:
        fig, ax = plt.subplots(figsize=(10, 8))
        correlation_matrix = df[numeric_columns].corr()
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=ax)
        ax.set_title('Correlation Matrix')
        
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()
        plt.close()
        
        charts.append({
            'title': 'Correlation Matrix',
            'type': 'heatmap',
            'url': f'data:image/png;base64,{image_base64}'
        })
    
    return charts

def generate_ai_insights(data_summary, analysis_results, goal):
    """Generate AI-powered insights using OpenAI"""
    try:
        prompt = f"""
        As a statistical analyst, provide insights for this dataset analysis:
        
        Goal: {goal}
        Dataset Summary: {json.dumps(data_summary, indent=2)}
        Analysis Results: {json.dumps(analysis_results, indent=2)}
        
        Please provide:
        1. A concise summary of the key findings
        2. 3-5 specific insights about the data
        3. Recommendations for further analysis
        4. Any potential data quality issues or limitations
        
        Keep the response professional and accessible to non-statisticians.
        """
        
        response =     client = OpenAI(api_key=current_app.config.get("OPENAI_API_KEY"))
    client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are an expert statistical analyst providing clear, actionable insights."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1000,
            temperature=0.3
        )
        
        return response.choices[0].message.content
    except Exception as e:
        return f"AI analysis unavailable: {str(e)}"

@statistical_bp.route('/upload', methods=['POST'])
def upload_file():
    """Handle file upload and return data preview"""
    if 'file' not in request.files:
        return jsonify({'error': 'No file provided'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No file selected'}), 400
    
    if not allowed_file(file.filename):
        return jsonify({'error': 'Invalid file type. Please upload CSV or Excel files.'}), 400
    
    try:
        # Read file based on extension
        filename = secure_filename(file.filename)
        if filename.endswith('.csv'):
            df = pd.read_csv(file)
        else:
            df = pd.read_excel(file)
        
        # Detect column types
        column_info = detect_column_types(df)
        
        # Generate preview
        preview_data = {
            'rowCount': len(df),
            'columnCount': len(df.columns),
            'columns': column_info,
            'sampleRows': df.head(10).fillna('').values.tolist()
        }
        
        # Store data in session (in production, use proper session management)
        session_id = f"session_{hash(str(df.values.tobytes()))}"
        
        return jsonify({
            'success': True,
            'sessionId': session_id,
            'preview': preview_data
        })
        
    except Exception as e:
        return jsonify({'error': f'Error processing file: {str(e)}'}), 500

@statistical_bp.route('/comprehensive', methods=['POST'])
def comprehensive_analysis():
    """Perform comprehensive statistical analysis"""
    try:
        # Get file and goal from request
        file = request.files.get('file')
        goal = request.form.get('goal', 'Describe / Summarize')
        
        if not file:
            return jsonify({'error': 'No file provided'}), 400
        
        # Read file
        filename = secure_filename(file.filename)
        if filename.endswith('.csv'):
            df = pd.read_csv(file)
        else:
            df = pd.read_excel(file)
        
        # Identify column types
        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
        categorical_columns = df.select_dtypes(include=['object']).columns.tolist()
        
        # Perform analyses
        descriptive_stats = calculate_descriptive_stats(df, numeric_columns)
        normality_tests = perform_normality_tests(df, numeric_columns)
        correlation_analysis = perform_correlation_analysis(df, numeric_columns)
        
        # Generate visualizations
        charts = generate_visualizations(df, numeric_columns, categorical_columns)
        
        # Prepare data summary for AI
        data_summary = {
            'rows': len(df),
            'columns': len(df.columns),
            'numeric_columns': len(numeric_columns),
            'categorical_columns': len(categorical_columns),
            'missing_values': df.isnull().sum().sum()
        }
        
        analysis_results = {
            'descriptive_stats': descriptive_stats,
            'normality_tests': normality_tests,
            'correlations': correlation_analysis
        }
        
        # Generate AI insights
        ai_summary = generate_ai_insights(data_summary, analysis_results, goal)
        
        # Prepare statistical tests based on goal
        statistical_tests = []
        
        if goal == 'Compare Groups' and len(categorical_columns) > 0 and len(numeric_columns) > 0:
            # Perform t-tests or ANOVA
            for cat_col in categorical_columns[:2]:  # Limit to first 2 categorical columns
                for num_col in numeric_columns[:2]:  # Limit to first 2 numeric columns
                    groups = df.groupby(cat_col)[num_col].apply(list)
                    if len(groups) == 2:
                        # Two-sample t-test
                        group1, group2 = groups.iloc[0], groups.iloc[1]
                        if len(group1) > 1 and len(group2) > 1:
                            t_stat, p_value = stats.ttest_ind(group1, group2)
                            statistical_tests.append({
                                'name': f'Two-sample t-test: {num_col} by {cat_col}',
                                'statistic': float(t_stat),
                                'pValue': float(p_value),
                                'result': 'Significant difference' if p_value < 0.05 else 'No significant difference',
                                'interpretation': f'Groups differ significantly (p={p_value:.4f})' if p_value < 0.05 else f'No significant difference between groups (p={p_value:.4f})'
                            })
        
        # Generate key findings
        key_findings = []
        
        if descriptive_stats:
            for col, stats_dict in descriptive_stats.items():
                if stats_dict['skewness'] > 1:
                    key_findings.append(f"{col} is highly right-skewed (skewness: {stats_dict['skewness']:.2f})")
                elif stats_dict['skewness'] < -1:
                    key_findings.append(f"{col} is highly left-skewed (skewness: {stats_dict['skewness']:.2f})")
                
                if stats_dict['std'] / stats_dict['mean'] > 0.5:
                    key_findings.append(f"{col} shows high variability (CV: {(stats_dict['std'] / stats_dict['mean']):.2f})")
        
        if correlation_analysis and 'pearson' in correlation_analysis:
            # Find strong correlations
            corr_matrix = correlation_analysis['pearson']
            for col1 in corr_matrix:
                for col2 in corr_matrix[col1]:
                    if col1 != col2 and abs(corr_matrix[col1][col2]) > 0.7:
                        key_findings.append(f"Strong correlation between {col1} and {col2} (r={corr_matrix[col1][col2]:.3f})")
        
        return jsonify({
            'success': True,
            'summary': ai_summary,
            'keyFindings': key_findings[:5],  # Limit to top 5 findings
            'descriptiveStats': descriptive_stats,
            'normalityTests': normality_tests,
            'correlations': correlation_analysis,
            'tests': statistical_tests,
            'charts': charts,
            'dataInfo': {
                'rows': len(df),
                'columns': len(df.columns),
                'numericColumns': numeric_columns,
                'categoricalColumns': categorical_columns
            }
        })
        
    except Exception as e:
        return jsonify({'error': f'Analysis error: {str(e)}'}), 500

@statistical_bp.route('/ai-chat', methods=['POST'])
def ai_chat():
    """Handle AI chat interactions"""
    try:
        data = request.get_json()
        message = data.get('message', '')
        data_context = data.get('dataContext', {})
        analysis_results = data.get('analysisResults', {})
        
        # Prepare context for AI
        context = f"""
        User has uploaded a dataset with:
        - {data_context.get('rowCount', 0)} rows
        - {data_context.get('columnCount', 0)} columns
        - Column types: {[col['name'] + ' (' + col['type'] + ')' for col in data_context.get('columns', [])]}
        
        Previous analysis results available: {bool(analysis_results)}
        
        User question: {message}
        """
        
        response =     client = OpenAI(api_key=current_app.config.get("OPENAI_API_KEY"))
    client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are a helpful statistical analysis assistant. Provide clear, actionable advice about statistical analysis. Suggest specific tests or analyses when appropriate."},
                {"role": "user", "content": context}
            ],
            max_tokens=500,
            temperature=0.3
        )
        
        ai_response = response.choices[0].message.content
        
        # Generate suggested actions based on the message
        suggested_actions = []
        
        if 'correlation' in message.lower():
            suggested_actions.append({
                'label': 'Run Correlation Analysis',
                'action': 'correlation_analysis',
                'description': 'Calculate Pearson and Spearman correlations'
            })
        
        if 'normal' in message.lower() or 'distribution' in message.lower():
            suggested_actions.append({
                'label': 'Test Normality',
                'action': 'normality_tests',
                'description': 'Run Shapiro-Wilk and other normality tests'
            })
        
        if 'compare' in message.lower() or 'difference' in message.lower():
            suggested_actions.append({
                'label': 'Compare Groups',
                'action': 'group_comparison',
                'description': 'Run t-tests or ANOVA to compare groups'
            })
        
        return jsonify({
            'success': True,
            'message': ai_response,
            'suggestedActions': suggested_actions
        })
        
    except Exception as e:
        return jsonify({'error': f'AI chat error: {str(e)}'}), 500

@statistical_bp.route('/execute-action', methods=['POST'])
def execute_action():
    """Execute suggested statistical actions"""
    try:
        data = request.get_json()
        action = data.get('action', {})
        data_context = data.get('dataContext', {})
        
        # This would need to access the stored dataset
        # For now, return a mock response
        
        action_type = action.get('action', '')
        
        if action_type == 'correlation_analysis':
            result = {
                'explanation': 'Correlation analysis completed. Strong positive correlation found between variables X and Y (r=0.85, p<0.001).',
                'statisticalResults': {
                    'test': 'Pearson Correlation',
                    'correlation_coefficient': 0.85,
                    'p_value': 0.001,
                    'interpretation': 'Strong positive correlation'
                }
            }
        elif action_type == 'normality_tests':
            result = {
                'explanation': 'Normality tests completed. Data appears to follow a normal distribution (Shapiro-Wilk p=0.12).',
                'statisticalResults': {
                    'test': 'Shapiro-Wilk',
                    'statistic': 0.95,
                    'p_value': 0.12,
                    'interpretation': 'Data is normally distributed'
                }
            }
        else:
            result = {
                'explanation': 'Analysis completed successfully.',
                'statisticalResults': {}
            }
        
        return jsonify({
            'success': True,
            **result
        })
        
    except Exception as e:
        return jsonify({'error': f'Action execution error: {str(e)}'}), 500
